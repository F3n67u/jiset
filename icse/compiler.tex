\section{Algorithm Compiler}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{img/algo_compiler.png}
  \caption{Overall structure of the algorithm compiler.}
  \label{fig:algo-compiler}
\end{figure}

In this section, we propose the \textit{algorithm compiler}
that compiles abstract algorithms into \( \ires \) functions
and its overall structure is described in Figure~\ref{fig:algo-compiler}.
Before compiling abstract algorithms,
the tokenizer first tokenizes each abstract algorithm into
a list of tagged tokens. Then, the token parser constructs
syntactic parse tree of the given token list.
Finally, the \( \ires \) function generator constructs the coressponding
\( \ires \) function from the given parse tree.
The parser and the generator is dependent on conversion rules
and we define the general conversion rules for abstract algorithms in
ECMAScript specifications. Like Coq, a proof assistant, we also provide
the \textit{rule generation assistant} to make easy write conversion rules.
It diagnoses root causes of failed parsing tokens
and suggests new conversion rules based on
statistical analysis of algorithm steps that failed to be parsed.

\subsection{Tokenizer}

Abstract algorithms in ECMAScript specifications are written in structured
natural languages in HTML files. An algorithm consists of ordered steps
and it might contain sub-steps as well. For example,
the \( \code{ToPrimitive} \) algorithm in Figure~\ref{fig:to-primitive}
has three steps and second step has seven sub-steps.
Moreover, the tokens of each step has its own HTML tag and each tag
has the following meaning:
\[
  \begin{array}{c|l}
    \text{HTML tags} & \text{meanings}\\\hline\hline
    \code{<var>} & \text{variables}\\\hline
    \code{<emu-grammar>} & \text{productions}\\\hline
    \code{<emu-nt>} & \text{non-terminal syntax}\\\hline
    \code{<code>} & \text{ECMAScript codes}\\\hline
    \code{<emu-const>} & \text{constant values}\\\hline
    \code{<emu-val>} & \text{values}\\\hline
    \code{<ol>} & \text{ordered sub-steps}\\\hline
    \code{<ul>} & \text{unordered sub-steps}\\\hline
    \code{<sup>} & \text{superscripts}\\\hline
    \text{otheriwse} & \text{simple texts}\\\hline
  \end{array}
\]
We try to keep the tag information for each token to generate more
precise conversion rules. For example, if and only if a token has a tag
\( \code{<var>} \), it is a parameter or a local variable.
Thus, it is possible to construct a conversion rule precisly discriminates
identifiers and other components.

The tokenizer first recognizes the overall structures of steps.
Then, it divides each step into sequence of tagged tokens.
If an HTML element has a explicit tag, it is converted into a single token
with the tag. Otherwise, it is splitted into multiple tokens and each token
should be a sequence of alphanumeric characters or a single
non-alphanumeric character. For example, in the \( \code{ToPrimitive} \)algorithm,
\( \textbf{\code{"default"}} \) is a single token with the tag \( \code{<code>} \)
and \( \code{@@toPrimitive} \) is splitted into three text tokens
\( \code{@} \), \( \code{@} \), and \( \code{toPrimitive} \).

For the linear structures, the tokenizer flatten the structured steps into
a single list. Some conversion rules should handle multiple steps such as
the conditional statements (if-then-else). Thus, we decide to break down
structured algorithms using three special tokens;
\( \tend \) denotes the end of a single step,
\( \tin \) and \( \tout \) the start and the end of nested steps, respectively.
For example, the \( \code{ToPrimitive} \) algorithm is tokenized as follows.
\[
  \begin{array}{l}
    \code{Assert} \cdots \tend\\
    \code{If} \cdots \tin \code{If} \cdots \tend
    \cdots \code{Return} \cdots \tend \tout \tend\\
    \code{Return} \cdots \tend\\
  \end{array}
\]

\subsection{Token Parser}

The token parser parses a given list of tokens into a syntactic parse tree.
It depends on the given \textit{conversion rules} that consists of
two parts parsing rules and mapping from parsing rules into corresponding
\( \ires \) components.
A parsing rule consists of basic token matchers and other parsing rules
including itself. For example, the following parsing rule is simplifed version for
sub-step \( i \) in the \( \code{ToPrimitive} \) algorithm.
\begin{lstlisting}[style=myScalastyle]
// statements
val Stmt = "Let" ~ varT ~ "be" ~ Expr ~ "."
// expressions
val Expr = (
  // identifiers
  varT |
  // return if abrupt
  "?" ~ Expr |
  // function calls
  textT ~ "(" ~ repsep(varT, ",") ~ ")" |
  // lists
  "<<" ~ repsep(Expr, ",") ~ ">>"
)
\end{lstlisting}

Each parsing rule is written in extended Scala parser combinators.
We modify the meaning of altenative composition operator ( \( | \) ) to collect
all longest matched results. If the parser detects that a step cannot be
parsed with the given parsing rules. It reports that a step is not possible
to be parsed under the given rules into the rule generation assistant.
Moreover, even though the parser could parse a step, it will be also reported
if it could be parsed in not a single but multiple ways.
Each string literal is a token matcher for tokens without any tags and checks
that the token has same string value with the sring literal.
The \( \code{varT} \) and \( \code{textT} \) is also token matchers
for token with the tag \( \code{<var>} \) and no tags.
The \( \code{Step} \) and \( \code{Expr} \) are user-defined parsing rules
defined with our extended parser combinators.
Morevoer, it supports all helpers functions defined in Scala parser combinators.
For example, the helper function \( \code{repsep(p, q)} \) generates a new
parsing rule that denotes zero or more repetitions of the parsing rule \( \code{p} \)
using another parsing rule \( \code{q} \) as separators.
Finally, the token parser with the above rules parses sub-step \( i \) in the
\( \code{ToPrimitive} \) into the following syntactic parse tree:
\includegraphics[width=0.5\textwidth]{img/parse_tree.png}

\subsection{\( \ires \) Function Generator}

The \( \ires \) function generator takes a parse tree from the token parser
and generates an \( \ires \) function. The given conversion rules describe
the mapping from each parsing rule into corresponding \( \ires \) components.
The \( \ires \) is an imperative style programming languages that we proposed
in order to represent each step of abstract algorithms in ECMAScript specifications.

% IR_ES 더 설명하기

\subsection{Rule Generation Assistant}
